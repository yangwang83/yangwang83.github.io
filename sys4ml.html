<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CSE 5439 Sys4ML</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px;
            background-color: #f4f7f6;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            background-color: white;
            box-shadow: 0 2px-5px rgba(0,0,0,0.1);
        }
        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #4CAF50;
            color: white;
            text-transform: uppercase;
            font-size: 14px;
        }
        tr:hover {
            background-color: #f5f5f5;
        }
    </style>
</head>
<body>

    <h2>Course schedule</h2>

    <table>
        <thead>
            <tr>
                <th>Date</th>
                <th>Topic</th>
                <th>Detail</th>
                <th>Presentor</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>1/13</td>
                <td>Introduction</td>
                <td> <a href="introduction.pdf">slides</a> </td>
                <td>Dr. Yang Wang</td>
            </tr>
           <tr>
                <td>1/15</td>
                <td>Framework</td>
                <td><a href="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf">TensorFlow: A System for Large-Scale Machine Learning</a> (OSDI 16)</td>
                <td> Shuzhan Yang </td>
            </tr>
            <tr>
                <td>1/15</td>
                <td>Framework</td>
                <td><a href="https://dl.acm.org/doi/10.5555/3454287.3455008">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a> (NIPS 19)</td>
                <td>Oliver Proudfoot</td>
            </tr>
            <tr>
                <td>1/20</td>
                <td>Framework</td>
                <td><a href="https://www.usenix.org/system/files/osdi18-moritz.pdf">Ray: A Distributed Framework for Emerging AI Applications</a> (OSDI 18)</td>
                <td>Jintong Liu</td>
            </tr>
            <tr>
                <td>1/20</td>
                <td>Parallelism</td>
                <td><a href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf">Scaling Distributed Machine Learning with the Parameter Server</a> (OSDI 14)</td>
                <td>Goutham Kuncham</td>
            </tr>
            <tr>
                <td>1/22</td>
                <td>Parallelism</td>
                <td><a href="https://arxiv.org/abs/1802.05799">Horovod: Fast and Easy Distributed Deep Learning in TensorFlow</a></td>
                <td>Kailun Lin</td>
            </tr>
            <tr>
                <td>1/22</td>
                <td>Parallelism</td>
                <td><a href="https://arxiv.org/abs/1909.08053">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></td>
                <td>Andy Wu</td>
            </tr>
            <tr>
                <td>1/27</td>
                <td>Snow day</td>
                <td></td>
                <td></td></td>
            </tr>
            <tr>
                <td>1/29</td>
                <td>Transformer</td>
                <td>Overview of the Transformer model</td>
                <td>Dr. Andrew Perrault</td>
            </tr>
            <tr>
                <td>2/3</td>
                <td>Parallelism</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3341301.3359646">PipeDream: generalized pipeline parallelism for DNN training</a> (SOSP 19)</td>
                <td>Qifan Yang</td>
            </tr>
            <tr>
                <td>2/3</td>
                <td>Parallelism</td>
                <td><a href="https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</a> (OSDI 22)</td>
                <td>Siyuan Zhang</td>
            </tr>
            <tr>
                <td>2/5</td>
                <td>Memory</td>
                <td><a href="https://arxiv.org/abs/1604.06174">Training Deep Nets with Sublinear Memory Cost</a></td>
                <td>Yuting Fang</td>
            </tr>
            <tr>
                <td>2/5</td>
                <td>Memory</td>
                <td><a href="https://dl.acm.org/doi/10.5555/3433701.3433727">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a> (SC 20)</td>
                <td>Sungjae Lee</td>
            </tr>
            <tr>
                <td>2/10</td>
                <td>Memory</td>
                <td><a href="https://www.usenix.org/system/files/atc21-ren-jie.pdf">ZeRO-Offload: Democratizing Billion-Scale Model Training</a> (USENIX ATC 21)</td>
                <td>William Cheng</td>
            </tr>
            <tr>
                <td>2/10</td>
                <td>Memory</td>
                <td><a href="https://dl.acm.org/doi/10.5555/3600270.3601459">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> (NIPS 21)</td>
                <td>Yingtie Lei</td>
            </tr>
            <tr>
                <td>2/12</td>
                <td>Compiler</td>
                <td><a href="https://www.usenix.org/system/files/osdi18-chen.pdf">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a> (OSDI 18)</td>
                <td>Siyuan Zhang</td>
            </tr>
            <tr>
                <td>2/12</td>
                <td>Compiler</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3315508.3329973">Triton: An Intermediate Language and Compiler for Tiled Neural Networks</a> (MAPL 19)</td>
                <td>Iris Kuo</td>
            </tr>
            <tr>
                <td>2/17</td>
                <td>Compiler</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3341301.3359630">TASO: optimizing deep learning computation with automatic generation of graph substitutions</a> (SOSP 19)</td>
                <td>Andy Wu</td>
            </tr>
            <tr>
                <td>2/17</td>
                <td>Compiler</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3575693.3576933">TensorIR: An Abstraction for Automatic Tensorized Program Optimization</a> (ASPLOS 23)</td>
                <td>Iris Kuo</td>
            </tr>
            <tr>
                <td>2/19</td>
                <td>Checkpoint</td>
                <td><a href="https://www.usenix.org/system/files/nsdi22-paper-eisenman.pdf">Check-N-Run: A Checkpointing System for Training Deep Learning Recommendation Models</a> (NSDI 22)</td>
                <td>Nick Cliffel</td>
            </tr>
            <tr>
                <td>2/19</td>
                <td>Checkpoint</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3600006.3613145">GEMINI: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints</a> (SOSP 23)</td>
                <td>Dylan Tan</td>
            </tr>
            <tr>
                <td>2/24</td>
                <td>Fault Tolerance</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3600006.3613152">Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates</a> (SOSP 23)</td>
                <td>Yingtie Lei</td>
            </tr>
            <tr>
                <td>2/24</td>
                <td>Fault Tolerance</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3694715.3695960">ReCycle: Resilient Training of Large DNNs using Pipeline Adaptation</a> (SOSP 24)</td>
                <td>Jintong Liu</td>
            </tr>
            <tr>
                <td>2/26</td>
                <td>Model Search</td>
                <td><a href="https://proceedings.mlr.press/v97/tan19a/tan19a.pdf?utm_medium=email&utm_source=transaction">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a> (ICML 19)</td>
                <td>Qifan Yang</td>
            </tr>
            <tr>
                <td>2/26</td>
                <td>Model Search</td>
                <td><a href="http://www.openreview.net/pdf?id=HylxE1HKwS">Once for All: Train One Network and Specialize it for Efficient Deployment</a> (ICLR 20)</td>
                <td>Fangxun Liu</td>
            </tr>
            <tr>
                <td>3/3</td>
                <td>Quantization</td>
                <td><a href="https://dl.acm.org/doi/10.5555/3600270.3602468">LLM.int8(): 8-bit matrix multiplication for transformers at scale</a> (NIPS 22)</td>
                <td>Hojin Yoo</td>
            </tr>
            <tr>
                <td>3/3</td>
                <td>Quantization</td>
                <td><a href="https://openreview.net/pdf?id=tcbBPnfwxS">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> (ICLR 23)</td>
                <td>Abeer Alshehri</td>
            </tr>
            <tr>
                <td>3/5</td>
                <td>Cluster</td>
                <td><a href="https://www.usenix.org/system/files/osdi18-xiao.pdf">Gandiva: Introspective Cluster Scheduling for Deep Learning</a> (OSDI 18)</td>
                <td>William Cheng</td>
            </tr>
            <tr>
                <td>3/5</td>
                <td>Cluster</td>
                <td><a href="https://www.usenix.org/system/files/atc19-jeon.pdf">Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads</a> (USENIX ATC 19)</td>
                <td>Goutham Kuncham</td>
            </tr>
            <tr>
                <td>3/10</td>
                <td>Cluster</td>
                <td><a href="https://www.usenix.org/system/files/nsdi19-gu.pdf">Tiresias: A GPU Cluster Manager for Distributed Deep Learning</a> (NSDI 19)</td>
                <td>Yao Lu</td>
            </tr>
            <tr>
                <td>3/10</td>
                <td>Cluster</td>
                <td><a href="https://www.usenix.org/system/files/nsdi20-paper-mahajan.pdf">Themis: Fair and Efficient GPU Cluster Scheduling</a> (NSDI 20)</td>
                <td>Srinivasan Subramaniyan</td>
            </tr>
            <tr>
                <td>3/12</td>
                <td>Cluster</td>
                <td><a href="https://www.usenix.org/system/files/osdi20-xiao.pdf">AntMan: Dynamic Scaling on GPU Clusters for Deep Learning</a> (OSDI 20)</td>
                <td>Chuyang Chen</td>
            </tr>
            <tr>
                <td>3/12</td>
                <td>Cluster</td>
                <td><a href="https://www.usenix.org/system/files/osdi21-qiao.pdf">Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning</a> (OSDI 21)</td>
                <td>Yuting Fang</td>
            </tr>
            <tr>
                <td>3/17</td>
                <td>Sprint break</td>
                <td></td>
                <td></td>
            </tr>
            <tr>
                <td>3/19</td>
                <td>Sprint break</td>
                <td></td>
                <td></td>
            </tr>
            <tr>
                <td>3/24</td>
                <td>Cluster</td>
                <td><a href="https://www.usenix.org/system/files/nsdi22-paper-weng.pdf">MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters</a> (NSDI 22)</td>
                <td>Srinivasan Subramaniyan</td>
            </tr>
            <tr>
                <td>3/24</td>
                <td>Cluster</td>
                <td><a href="https://www.usenix.org/system/files/osdi24-choudhury.pdf">MAST: global scheduling of ML training across geo-distributed datacenters at hyperscale</a> (OSDI 24)</td>
                <td>Dylan Tan</td>
            </tr>
            <tr>
                <td>3/26</td>
                <td>Inference</td>
                <td><a href="https://arxiv.org/abs/1712.06139">TensorFlow-Serving: Flexible, High-Performance ML Serving</a></td>
                <td>Abeer Alshehri</td>
            </tr>
            <tr>
                <td>3/26</td>
                <td>Inference</td>
                <td><a href="https://www.usenix.org/system/files/osdi20-gujarati.pdf">Serving DNNs like Clockwork: Performance Predictability from the Bottom Up</a> (OSDI 20)</td>
                <td>Yao Lu</td>
            </tr>
            <tr>
                <td>3/31</td>
                <td>Inference</td>
                <td><a href="https://dl.acm.org/doi/abs/10.5555/3571885.3571946">DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale</a> (SC 22)</td>
                <td>Fangxun Liu</td>
            </tr>
            <tr>
                <td>3/31</td>
                <td>Inference</td>
                <td><a href="https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf">DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</a> (ICML 22)</td>
                <td>Sungjae Lee</td>
            </tr>
            <tr>
                <td>4/2</td>
                <td>Inference</td>
                <td><a href="https://www.usenix.org/system/files/osdi22-yu.pdf">Orca: A Distributed Serving System for Transformer</a> (OSDI 22)</td>
                <td>Yuan Ma</td>
            </tr>
            <tr>
                <td>4/2</td>
                <td>Inference</td>
                <td><a href="https://dl.acm.org/doi/10.1145/3600006.3613165">Efficient Memory Management for LLM Serving with PagedAttention</a> (SOSP 23)</td>
                <td>Hojin Yoo</td>
            </tr>
            <tr>
                <td>4/7</td>
                <td>Inference</td>
                <td><a href="https://www.usenix.org/system/files/osdi23-li-zhuohan.pdf">AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving</a> (OSDI 23)</td>
                <td>Kailun Lin</td>
            </tr>
            <tr>
                <td>4/7</td>
                <td>Multimodal</td>
                <td><a href="https://www.usenix.org/system/files/nsdi24-huang.pdf">DistMM: Accelerating Distributed Multimodal  Model Training</a></td>
                <td>Invited speaker: Jun Huang</td>
            </tr>
            <tr>
                <td>4/9</td>
                <td>Inference</td>
                <td><a href="https://dl.acm.org/doi/abs/10.5555/3618408.3619696">FlexGen: high-throughput generative inference of large language models with a single GPU</a> (ICML 23)</td>
                <td>Shuzhan Yang</td>
            </tr>
            <tr>
                <td>4/9</td>
                <td>Inference</td>
                <td><a href="https://www.usenix.org/system/files/osdi24-zhong-yinmin.pdf">DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</a> (OSDI 24)</td>
                <td>Oliver Proudfoot</td>
            </tr>
            <tr>
                <td>4/14</td>
                <td>TBD</td>
                <td>TBD</td>
                <td>?</td>
            </tr>
            <tr>
                <td>4/16</td>
                <td>Project Presentation</td>
                <td>?</td>
                <td>?</td>
            </tr>
            <tr>
                <td>4/21</td>
                <td>Project Presentation</td>
                <td>?</td>
                <td>?</td>
            </tr>
            <tr>
                <td>4/23</td>
                <td>Project Presentation</td>
                <td>?</td>
                <td>?</td>
            </tr>
        </tbody>
    </table>

</body>
</html>
